
# 🧠 ML-Architectures

**ML-Architectures** is a structured knowledge project designed to deeply explain how classic machine learning models work — from their internal mathematical structures, loss functions, optimization processes, to complete training pipelines.

This project helps you understand:
- What exactly a model like Linear Regression, SVM, or XGBoost is doing internally
- How gradient-based optimization adjusts parameters
- How to build, train, and evaluate models from scratch
- The core intuition behind loss functions and optimizers

---


## 📦 What’s inside

```
ML-Architectures/
├── models/                 # Each ML model has its own module
│   ├── linear_regression/
│   ├── logistic_regression/
│   ├── decision_tree/
│   ├── random_forest/
│   ├── xgboost/
│   ├── knn/
│   ├── svm/
│   ├── kmeans/
│   └── neural_network/
├── optimizers/             # Optimization methods explained
│   ├── gradient_descent.md
│   ├── sgd.md
│   ├── adam.md
│   └── comparison.md
├── notes/                  # Key concepts: loss functions, regularization, etc.
│   ├── loss_functions.md
│   ├── regularization.md
│   └── training_pipeline.md
├── diagrams/               # Visual explanations, architecture charts
├── LICENSE
└── README.md
```

---

## 🎯 Target audience

- ML learners who want to go beyond APIs and understand what’s under the hood
- Aspiring researchers, data scientists, or quant developers
- Interview candidates preparing for deep algorithm/model questions

---

## 📘 Models covered (will expand weekly)

- Linear Regression (✅ in progress)
- Logistic Regression
- Decision Tree & Random Forest
- KNN
- XGBoost / LightGBM
- Support Vector Machines
- KMeans (unsupervised)
- Neural Networks (basic MLP)

---

## ⚙️ Tools

- Core math implemented in Python/Numpy
- Visualizations via Matplotlib & Plotly
- Optional PyTorch modules for more complex experiments

---

## 🧠 Vision

> To build a model-by-model, optimizer-by-optimizer understanding of machine learning from the ground up — as simply, deeply, and reproducibly as possible.

---

## 📄 License

This project is licensed under the MIT License.  
© 2025 Pengyi Peng. All rights reserved.
